---
title: "endangered_species_pull"
author: "Marina Goldgisser"
date: "8/2/2021"
output: html_document
---
# Download data from GBIF

## Libraries and setup

```{r load libraries, warning=FALSE}

library(tidyverse)
library(rgbif) # for occ_download
library(taxize) # for get_gbifid_

user<- "mgoldy"
email <- "marina.goldgisser@gmail.com"
pwd <- "ecoblenderlab"

setwd("~/GitHub/desert-fires-impact-biodiversity")

list.csv <- "~/GitHub/desert-fires-impact-biodiversity/data/archive/Original-data/endangered-species-list.csv" #csv with list of species 

```

## download occurrence data from gbif

Since we aren't going to with on GBIF data directly in the portal, we have to download a subset to work on in R. The uses a csv file with TaxonName (genus species) to pull GBIF data from the portal using a polygon geometry.

```{r}

gbif_taxon_keys <- 
  readr::read_csv(list.csv) %>% 
  pull("TaxonName") %>% 
  taxize::get_gbifid_(method="backbone") %>% # match names to the GBIF backbone to get taxonkeys
  imap(~ .x %>% mutate(original_sciname = .y)) %>% # add original name back into data.frame
  bind_rows() %T>% # combine all data.frames into one
  readr::write_csv(path = "~/GitHub/desert-fires-impact-biodiversity/data/archive/Original-data/all_matches.csv") %>% # save as side effect for you to inspect if you want
  filter(matchtype == "EXACT" & (status == "ACCEPTED" | status == "SYNONYM")) %>% # get only accepted and matched names
  filter(kingdom == "Animalia") %>% # remove anything that might have matched to a non-animal
  pull(usagekey)

#check all animal species included; this is done by comparing the match-check to the csv

gbifids <- read_csv("~/GitHub/desert-fires-impact-biodiversity/data/archive/Original-data/all_matches.csv") %>% 
  filter(usagekey %in% gbif_taxon_keys)

match_check <- read_csv(list.csv) %>% 
  filter(!species %in% gbifids$original_sciname)

#append species missed

gbif_taxon_keys <- append(gbif_taxon_keys, 6091918)


# gbif_taxon_keys should be a long vector like this c(2977832,2977901,2977966,2977835,2977863)

# !!very important - Use pred_in to filter out occurrence data!!

occ_download(
  pred_in("taxonKey", gbif_taxon_keys),
  pred_in("basisOfRecord", c('MACHINE_OBSERVATION', 'HUMAN_OBSERVATION','OBSERVATION', 'MATERIAL_SAMPLE', 'LITERATURE')),
  pred_within("POLYGON((-123.36768 32.60236,-113.62939 32.60236,-113.62939 38.21401,-123.36768 38.21401,-123.36768 32.60236))"), #this polygon is a square Nborder-SanFran Sborder-mexico E/Wborder-include all CA
  pred("hasCoordinate", TRUE),
  pred("hasGeospatialIssue", FALSE),
  pred_gte("year", 1994),
  pred_lte('year', 2021),
  format = "SIMPLE_CSV",
  user=user,pwd=pwd,email=email
)
```
2 Aug 2021 - Download key: 0336745-200613084148143


27 Jan 2022 - <<gbif download>>
  Username: mgoldy
  E-mail: marina.goldgisser@gmail.com
  Format: SIMPLE_CSV
  Download key: 0117084-210914110416597
  
9 May 2022 <<gbif download>> (made sure all species included in search)
  Username: mgoldy
  E-mail: marina.goldgisser@gmail.com
  Format: SIMPLE_CSV
  Download key: 0272564-210914110416597



#filter observation data by fire shapefile


```{r warning=FALSE}
library(tidyverse)
library(sf)
library(rgdal)
library(raster)
library(rgeos)
library(here)

gbifdata <- read_csv(here("endangered", "data", "0336745-200613084148143.csv")) 



#remove columns we don't need
gbif <- subset(gbifdata, select = c("verbatimScientificName","class", "order", "year", "month", "day","decimalLongitude", "decimalLatitude", "individualCount", "basisOfRecord")) %>% 
  rename("species" = "verbatimScientificName")
  

#transform to shapefile and add projection
gbif_sf <- st_as_sf(gbif, coords = c("decimalLongitude", "decimalLatitude"), crs = 4326, remove = FALSE)

#add obs ID
id <- rownames(gbif_sf) 
gbif_sf <- cbind(obsID=id, gbif_sf)

#save gbif observations as shapefile for later use
st_write(gbif_sf, here("endangered", "data", "endangeredanimals"), driver = "ESRI Shapefile", delete_layer = TRUE)

# add gbif shapefile; this is needed if already converted the gbif csv to shapefile
gbif_sf <- st_read(here("endangered", "data", "endangeredanimals")) 


#add fire shapefile
fire_sf <- st_read("./shapefiles/fireshapefile") %>% 
  st_make_valid(fire_sf)

#not needed, but this transforms the projections so that they are the same
# gbif_sf <- st_transform(gbif_sf, st_crs(fire_sf))

#spatial aggregation: observation points to fire polygons
filter_gbif <- st_filter(gbif_sf, fire_sf, .predicate = st_within)
join_fire_gbif <- st_join(filter_gbif, fire_sf, join = st_within) %>% 
  st_drop_geometry() %>% 
  rename(fireyear = FIRE_YEARn, firename = INCIDENT, firesize = GISAcres)

#counts up multiple instances and generates new vector as counts by grouping factors####
fire_total_obs <- join_fire_gbif %>% 
  group_by(fireID, firename, fireyear, firesize) %>% 
  summarize(n_observations = n()) 
  
write_csv(fire_total_obs, here("endangered", "data", "bronze.csv"))
write_csv(join_fire_gbif, here("endangered", "data", "gold2.csv"))

```